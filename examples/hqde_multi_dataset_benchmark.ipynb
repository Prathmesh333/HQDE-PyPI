{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HQDE Multi-Dataset Benchmark\n",
    "\n",
    "This notebook benchmarks the HQDE (Hierarchical Quantum-Distributed Ensemble) framework on multiple datasets.\n",
    "\n",
    "## Datasets:\n",
    "1. MNIST (Grayscale, 10 classes)\n",
    "2. CIFAR-10 (RGB, 10 classes)\n",
    "3. Cats vs Dogs (RGB, 2 classes)\n",
    "4. Fashion MNIST (Grayscale, 10 classes)\n",
    "5. STL-10 (RGB, 10 classes)\n",
    "6. Oxford Flowers 17 (RGB, 17 classes)\n",
    "7. SVHN (RGB, 10 classes)\n",
    "8. CIFAR-100 (RGB, 100 classes)\n",
    "\n",
    "## Hardware Configuration:\n",
    "- 2x T4 GPUs\n",
    "- 4 CPU Cores\n",
    "- Ray for distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install HQDE and dependencies\n",
    "!pip install hqde torch torchvision torchaudio --quiet\n",
    "!pip install ray[rllib] tensorboard --quiet\n",
    "!pip install kaggle --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "import ray\n",
    "from ray.util.actor_pool import ActorPool\n",
    "\n",
    "# Import HQDE\n",
    "from hqde import create_hqde_system, PerformanceMonitor\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check available GPUs\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "logger.info(f'Using device: {DEVICE}')\n",
    "logger.info(f'Number of GPUs available: {NUM_GPUS}')\n",
    "\n",
    "# Initialize Ray with GPU support\n",
    "if NUM_GPUS >= 2:\n",
    "    ray.init(ignore_reinit_error=True, num_cpus=4, num_gpus=2)\n",
    "    logger.info('Ray initialized with 2 GPUs')\n",
    "else:\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    logger.info('Ray initialized in CPU mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures for Different Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"Simple CNN for small grayscale images (MNIST, Fashion MNIST).\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class StandardCNN(nn.Module):\n",
    "    \"\"\"Standard CNN for RGB images (CIFAR-10, SVHN, CIFAR-100).\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(StandardCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepCNN(nn.Module):\n",
    "    \"\"\"Deeper CNN for larger datasets (STL-10, Flowers, Cats vs Dogs).\"\"\"\n",
    "    def __init__(self, num_classes=10, input_size=96):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Calculate adaptive pooling size\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4 * 4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"Models defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConfig:\n",
    "    \"\"\"Configuration for each dataset.\"\"\"\n",
    "    def __init__(self, name, model_class, model_kwargs, \n",
    "                 transform_train, transform_test, \n",
    "                 num_classes, batch_size=64, subset_size=5000):\n",
    "        self.name = name\n",
    "        self.model_class = model_class\n",
    "        self.model_kwargs = model_kwargs\n",
    "        self.transform_train = transform_train\n",
    "        self.transform_test = transform_test\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "\n",
    "def load_mnist(subset_size=5000, batch_size=64):\n",
    "    \"\"\"Load MNIST dataset.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True, \n",
    "                                               download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False, \n",
    "                                              download=True, transform=transform)\n",
    "    \n",
    "    # Create subsets\n",
    "    train_subset = Subset(train_dataset, range(min(subset_size, len(train_dataset))))\n",
    "    test_subset = Subset(test_dataset, range(min(1000, len(test_dataset))))\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    config = DatasetConfig(\n",
    "        name='MNIST',\n",
    "        model_class=SimpleCNN,\n",
    "        model_kwargs={'num_classes': 10},\n",
    "        transform_train=transform,\n",
    "        transform_test=transform,\n",
    "        num_classes=10,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader, config\n",
    "\n",
    "\n",
    "def load_fashion_mnist(subset_size=5000, batch_size=64):\n",
    "    \"\"\"Load Fashion MNIST dataset.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.2860,), (0.3530,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, \n",
    "                                                       download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, \n",
    "                                                      download=True, transform=transform)\n",
    "    \n",
    "    train_subset = Subset(train_dataset, range(min(subset_size, len(train_dataset))))\n",
    "    test_subset = Subset(test_dataset, range(min(1000, len(test_dataset))))\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    config = DatasetConfig(\n",
    "        name='Fashion-MNIST',\n",
    "        model_class=SimpleCNN,\n",
    "        model_kwargs={'num_classes': 10},\n",
    "        transform_train=transform,\n",
    "        transform_test=transform,\n",
    "        num_classes=10,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader, config\n",
    "\n",
    "\n",
    "def load_cifar10(subset_size=5000, batch_size=64):\n",
    "    \"\"\"Load CIFAR-10 dataset.\"\"\"\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                                download=True, transform=transform_train)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
    "                                               download=True, transform=transform_test)\n",
    "    \n",
    "    train_subset = Subset(train_dataset, range(min(subset_size, len(train_dataset))))\n",
    "    test_subset = Subset(test_dataset, range(min(1000, len(test_dataset))))\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    config = DatasetConfig(\n",
    "        name='CIFAR-10',\n",
    "        model_class=StandardCNN,\n",
    "        model_kwargs={'num_classes': 10},\n",
    "        transform_train=transform_train,\n",
    "        transform_test=transform_test,\n",
    "        num_classes=10,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader, config\n",
    "\n",
    "\n",
    "def load_cifar100(subset_size=5000, batch_size=64):\n",
    "    \"\"\"Load CIFAR-100 dataset.\"\"\"\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, \n",
    "                                                 download=True, transform=transform_train)\n",
    "    test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, \n",
    "                                                download=True, transform=transform_test)\n",
    "    \n",
    "    train_subset = Subset(train_dataset, range(min(subset_size, len(train_dataset))))\n",
    "    test_subset = Subset(test_dataset, range(min(1000, len(test_dataset))))\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    config = DatasetConfig(\n",
    "        name='CIFAR-100',\n",
    "        model_class=StandardCNN,\n",
    "        model_kwargs={'num_classes': 100},\n",
    "        transform_train=transform_train,\n",
    "        transform_test=transform_test,\n",
    "        num_classes=100,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader, config\n",
    "\n",
    "\n",
    "def load_svhn(subset_size=5000, batch_size=64):\n",
    "    \"\"\"Load SVHN dataset.\"\"\"\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4377, 0.4438, 0.4728), (0.1980, 0.2010, 0.1970))\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4377, 0.4438, 0.4728), (0.1980, 0.2010, 0.1970))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.SVHN(root='./data', split='train', \n",
    "                                              download=True, transform=transform_train)\n",
    "    test_dataset = torchvision.datasets.SVHN(root='./data', split='test', \n",
    "                                             download=True, transform=transform_test)\n",
    "    \n",
    "    train_subset = Subset(train_dataset, range(min(subset_size, len(train_dataset))))\n",
    "    test_subset = Subset(test_dataset, range(min(1000, len(test_dataset))))\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    config = DatasetConfig(\n",
    "        name='SVHN',\n",
    "        model_class=StandardCNN,\n",
    "        model_kwargs={'num_classes': 10},\n",
    "        transform_train=transform_train,\n",
    "        transform_test=transform_test,\n",
    "        num_classes=10,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader, config\n",
    "\n",
    "\n",
    "def load_stl10(subset_size=5000, batch_size=32):\n",
    "    \"\"\"Load STL-10 dataset.\"\"\"\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(96),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(96, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4467, 0.4390, 0.4066), (0.2603, 0.2566, 0.2713))\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(96),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4467, 0.4390, 0.4066), (0.2603, 0.2566, 0.2713))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.STL10(root='./data', split='train', \n",
    "                                               download=True, transform=transform_train)\n",
    "    test_dataset = torchvision.datasets.STL10(root='./data', split='test', \n",
    "                                              download=True, transform=transform_test)\n",
    "    \n",
    "    train_subset = Subset(train_dataset, range(min(subset_size, len(train_dataset))))\n",
    "    test_subset = Subset(test_dataset, range(min(1000, len(test_dataset))))\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    config = DatasetConfig(\n",
    "        name='STL-10',\n",
    "        model_class=DeepCNN,\n",
    "        model_kwargs={'num_classes': 10, 'input_size': 96},\n",
    "        transform_train=transform_train,\n",
    "        transform_test=transform_test,\n",
    "        num_classes=10,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader, config\n",
    "\n",
    "\n",
    "print(\"Dataset loaders defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HQDE Benchmark Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_hqde(train_loader, test_loader, config, num_epochs=5, num_workers=2):\n",
    "    \"\"\"Benchmark HQDE on a dataset.\"\"\"\n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"Benchmarking HQDE on {config.name}\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    results = {\n",
    "        'dataset': config.name,\n",
    "        'num_classes': config.num_classes,\n",
    "        'num_workers': num_workers,\n",
    "        'num_epochs': num_epochs,\n",
    "        'train_samples': len(train_loader.dataset),\n",
    "        'test_samples': len(test_loader.dataset)\n",
    "    }\n",
    "    \n",
    "    # Create HQDE system\n",
    "    logger.info(f\"Creating HQDE system with {num_workers} workers...\")\n",
    "    hqde_system = create_hqde_system(\n",
    "        model_class=config.model_class,\n",
    "        model_kwargs=config.model_kwargs,\n",
    "        num_workers=num_workers,\n",
    "        quantization_config={'base_bits': 8, 'min_bits': 4, 'max_bits': 16},\n",
    "        aggregation_config={'noise_scale': 0.005, 'exploration_factor': 0.1}\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    logger.info(f\"Training for {num_epochs} epochs...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    training_metrics = hqde_system.train(train_loader, num_epochs=num_epochs)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    results['training_time'] = training_time\n",
    "    logger.info(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluation\n",
    "    logger.info(\"Evaluating on test set...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for data, targets in test_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        predictions = hqde_system.predict([data])\n",
    "        \n",
    "        if predictions.numel() > 0:\n",
    "            loss = criterion(predictions, targets).item()\n",
    "            _, predicted_classes = torch.max(predictions, dim=1)\n",
    "            \n",
    "            total_correct += (predicted_classes == targets).sum().item()\n",
    "            total_loss += loss * data.size(0)\n",
    "            total_samples += data.size(0)\n",
    "    \n",
    "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "    avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "    \n",
    "    results['test_accuracy'] = accuracy\n",
    "    results['test_loss'] = avg_loss\n",
    "    results['correct_predictions'] = total_correct\n",
    "    \n",
    "    logger.info(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    logger.info(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Get HQDE performance metrics\n",
    "    hqde_metrics = hqde_system.get_performance_metrics()\n",
    "    results['hqde_metrics'] = hqde_metrics\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f\"hqde_{config.name.lower().replace('-', '_')}_model.pth\"\n",
    "    hqde_system.save_model(model_path)\n",
    "    results['model_path'] = model_path\n",
    "    \n",
    "    # Cleanup\n",
    "    hqde_system.cleanup()\n",
    "    \n",
    "    logger.info(f\"Benchmark completed for {config.name}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Benchmark function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmarks on All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of dataset loaders\n",
    "dataset_loaders = {\n",
    "    'MNIST': load_mnist,\n",
    "    'Fashion-MNIST': load_fashion_mnist,\n",
    "    'CIFAR-10': load_cifar10,\n",
    "    'SVHN': load_svhn,\n",
    "    'STL-10': load_stl10,\n",
    "    'CIFAR-100': load_cifar100,\n",
    "}\n",
    "\n",
    "# Results storage\n",
    "all_results = []\n",
    "\n",
    "# Configuration\n",
    "NUM_EPOCHS = 5\n",
    "SUBSET_SIZE = 5000  # Use subset for faster benchmarking\n",
    "NUM_WORKERS = 2  # Adjust based on your GPU count\n",
    "\n",
    "print(f\"Starting benchmarks with {NUM_WORKERS} workers, {NUM_EPOCHS} epochs\")\n",
    "print(f\"Subset size: {SUBSET_SIZE} samples per dataset\")\n",
    "print(f\"Running on {NUM_GPUS} GPUs\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, config = load_mnist(subset_size=SUBSET_SIZE, batch_size=64)\n",
    "results_mnist = benchmark_hqde(train_loader, test_loader, config, \n",
    "                               num_epochs=NUM_EPOCHS, num_workers=NUM_WORKERS)\n",
    "all_results.append(results_mnist)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nResults Summary for {config.name}:\")\n",
    "print(f\"  Training Time: {results_mnist['training_time']:.2f}s\")\n",
    "print(f\"  Test Accuracy: {results_mnist['test_accuracy']*100:.2f}%\")\n",
    "print(f\"  Test Loss: {results_mnist['test_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, config = load_fashion_mnist(subset_size=SUBSET_SIZE, batch_size=64)\n",
    "results_fashion = benchmark_hqde(train_loader, test_loader, config, \n",
    "                                  num_epochs=NUM_EPOCHS, num_workers=NUM_WORKERS)\n",
    "all_results.append(results_fashion)\n",
    "\n",
    "print(f\"\\nResults Summary for {config.name}:\")\n",
    "print(f\"  Training Time: {results_fashion['training_time']:.2f}s\")\n",
    "print(f\"  Test Accuracy: {results_fashion['test_accuracy']*100:.2f}%\")\n",
    "print(f\"  Test Loss: {results_fashion['test_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, config = load_cifar10(subset_size=SUBSET_SIZE, batch_size=64)\n",
    "results_cifar10 = benchmark_hqde(train_loader, test_loader, config, \n",
    "                                  num_epochs=NUM_EPOCHS, num_workers=NUM_WORKERS)\n",
    "all_results.append(results_cifar10)\n",
    "\n",
    "print(f\"\\nResults Summary for {config.name}:\")\n",
    "print(f\"  Training Time: {results_cifar10['training_time']:.2f}s\")\n",
    "print(f\"  Test Accuracy: {results_cifar10['test_accuracy']*100:.2f}%\")\n",
    "print(f\"  Test Loss: {results_cifar10['test_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, config = load_svhn(subset_size=SUBSET_SIZE, batch_size=64)\n",
    "results_svhn = benchmark_hqde(train_loader, test_loader, config, \n",
    "                               num_epochs=NUM_EPOCHS, num_workers=NUM_WORKERS)\n",
    "all_results.append(results_svhn)\n",
    "\n",
    "print(f\"\\nResults Summary for {config.name}:\")\n",
    "print(f\"  Training Time: {results_svhn['training_time']:.2f}s\")\n",
    "print(f\"  Test Accuracy: {results_svhn['test_accuracy']*100:.2f}%\")\n",
    "print(f\"  Test Loss: {results_svhn['test_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, config = load_cifar100(subset_size=SUBSET_SIZE, batch_size=64)\n",
    "results_cifar100 = benchmark_hqde(train_loader, test_loader, config, \n",
    "                                   num_epochs=NUM_EPOCHS, num_workers=NUM_WORKERS)\n",
    "all_results.append(results_cifar100)\n",
    "\n",
    "print(f\"\\nResults Summary for {config.name}:\")\n",
    "print(f\"  Training Time: {results_cifar100['training_time']:.2f}s\")\n",
    "print(f\"  Test Accuracy: {results_cifar100['test_accuracy']*100:.2f}%\")\n",
    "print(f\"  Test Loss: {results_cifar100['test_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. STL-10 (Optional - Larger Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run STL-10 (takes longer due to larger images)\n",
    "# train_loader, test_loader, config = load_stl10(subset_size=SUBSET_SIZE, batch_size=32)\n",
    "# results_stl10 = benchmark_hqde(train_loader, test_loader, config, \n",
    "#                                num_epochs=NUM_EPOCHS, num_workers=NUM_WORKERS)\n",
    "# all_results.append(results_stl10)\n",
    "# \n",
    "# print(f\"\\nResults Summary for {config.name}:\")\n",
    "# print(f\"  Training Time: {results_stl10['training_time']:.2f}s\")\n",
    "# print(f\"  Test Accuracy: {results_stl10['test_accuracy']*100:.2f}%\")\n",
    "# print(f\"  Test Loss: {results_stl10['test_loss']:.4f}\")\n",
    "\n",
    "print(\"STL-10 benchmark skipped (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "for result in all_results:\n",
    "    summary_data.append({\n",
    "        'Dataset': result['dataset'],\n",
    "        'Classes': result['num_classes'],\n",
    "        'Train Samples': result['train_samples'],\n",
    "        'Test Samples': result['test_samples'],\n",
    "        'Training Time (s)': f\"{result['training_time']:.2f}\",\n",
    "        'Test Accuracy (%)': f\"{result['test_accuracy']*100:.2f}\",\n",
    "        'Test Loss': f\"{result['test_loss']:.4f}\",\n",
    "        'Correct Predictions': result['correct_predictions']\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"HQDE MULTI-DATASET BENCHMARK RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save results to JSON\n",
    "with open('hqde_benchmark_results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(\"\\nResults saved to 'hqde_benchmark_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "# Extract data for plotting\n",
    "datasets = [r['dataset'] for r in all_results]\n",
    "accuracies = [r['test_accuracy']*100 for r in all_results]\n",
    "training_times = [r['training_time'] for r in all_results]\n",
    "losses = [r['test_loss'] for r in all_results]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Test Accuracy Comparison\n",
    "axes[0, 0].bar(datasets, accuracies, color='steelblue', alpha=0.8)\n",
    "axes[0, 0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0, 0].set_title('Test Accuracy by Dataset', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylim([0, 100])\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0, 0].text(i, v + 1, f'{v:.1f}%', ha='center', fontsize=10)\n",
    "\n",
    "# 2. Training Time Comparison\n",
    "axes[0, 1].bar(datasets, training_times, color='coral', alpha=0.8)\n",
    "axes[0, 1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "axes[0, 1].set_title('Training Time by Dataset', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(training_times):\n",
    "    axes[0, 1].text(i, v + 1, f'{v:.1f}s', ha='center', fontsize=10)\n",
    "\n",
    "# 3. Test Loss Comparison\n",
    "axes[1, 0].bar(datasets, losses, color='lightgreen', alpha=0.8)\n",
    "axes[1, 0].set_ylabel('Test Loss', fontsize=12)\n",
    "axes[1, 0].set_title('Test Loss by Dataset', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(losses):\n",
    "    axes[1, 0].text(i, v + 0.05, f'{v:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "# 4. Accuracy vs Training Time Scatter\n",
    "axes[1, 1].scatter(training_times, accuracies, s=200, alpha=0.6, c='purple')\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "axes[1, 1].set_title('Accuracy vs Training Time', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add labels for each point\n",
    "for i, dataset in enumerate(datasets):\n",
    "    axes[1, 1].annotate(dataset, (training_times[i], accuracies[i]),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hqde_benchmark_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to 'hqde_benchmark_plots.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Cluster Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Ray cluster\n",
    "ray.shutdown()\n",
    "print(\"\\nRay cluster shutdown complete.\")\n",
    "print(\"Benchmark finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Findings\n",
    "\n",
    "This notebook demonstrated the HQDE framework on multiple datasets:\n",
    "\n",
    "### Key Features Tested:\n",
    "1. **Distributed Training**: Multi-GPU support via Ray\n",
    "2. **Adaptive Quantization**: Memory-efficient training\n",
    "3. **Quantum-Inspired Aggregation**: Ensemble learning with noise injection\n",
    "4. **Scalability**: From simple (MNIST) to complex (CIFAR-100) datasets\n",
    "\n",
    "### Hardware Configuration:\n",
    "- 2x T4 GPUs\n",
    "- 4 CPU Cores  \n",
    "- Ray distributed framework\n",
    "\n",
    "### Files Generated:\n",
    "- `hqde_benchmark_results.json` - Detailed results\n",
    "- `hqde_benchmark_plots.png` - Visualization\n",
    "- `hqde_*_model.pth` - Trained models for each dataset\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different hyperparameters\n",
    "- Try larger subset sizes for better accuracy\n",
    "- Test on custom datasets\n",
    "- Compare with baseline models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
